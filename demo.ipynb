{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(42)\n",
    "\n",
    "data = json.load(open(\"data/input/news-60k-2023-cleaned.json\"))\n",
    "shuffle(data)\n",
    "subset = data[:100]\n",
    "shuffle\n",
    "for i, item in enumerate(subset):\n",
    "    item[\"text\"] = item[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/input/news-100.jsonl\", \"w\") as f:\n",
    "#     for item in subset:\n",
    "#         f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "json.dump(subset, open(\"data/input/news-100.json\", \"w\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopicGPT_Python package\n",
    "\n",
    "`topicgpt_python` consists of five modules in total: \n",
    "- `generate_topic_lvl1` generates high-level and generalizable topics. \n",
    "- `generate_topic_lvl2` generates low-level and specific topics to each high-level topic.\n",
    "- `refine_topics` refines the generated topics by merging similar topics and removing irrelevant topics.\n",
    "- `assign_topics` assigns the generated topics to the input text, along with a quote that supports the assignment.\n",
    "- `correct_topics` corrects the generated topics by reprompting the model so that the topic assignment is grounded in the topic list. \n",
    "\n",
    "![topicgpt_python](assets/img/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Make a new Python 3.9+ environment using virtualenv or conda. \n",
    "2. Install the required packages: `pip install --upgrade topicgpt_python`.\n",
    "- Our package supports OpenAI API, Google Cloud Vertex AI API, Gemini API, Azure API, and vLLM inference. vLLM requires GPUs to run. \n",
    "- Please refer to https://openai.com/pricing/ for OpenAI API pricing or to https://cloud.google.com/vertex-ai/pricing for Vertex API pricing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run in shell\n",
    "!pip install --upgrade topicgpt_python\n",
    "\n",
    "# Needed only for the OpenAI API deployment\n",
    "export OPENAI_API_KEY={your_openai_api_key}\n",
    "\n",
    "# Needed only for the Vertex AI deployment\n",
    "export VERTEX_PROJECT={your_vertex_project}   # e.g. my-project\n",
    "export VERTEX_LOCATION={your_vertex_location} # e.g. us-central1\n",
    "\n",
    "# Needed only for Gemini deployment\n",
    "export GEMINI_API_KEY={your_gemini_api_key}\n",
    "\n",
    "# Needed only for the Azure API deployment\n",
    "export AZURE_OPENAI_API_KEY={your_azure_api_key}\n",
    "export AZURE_OPENAI_ENDPOINT={your_azure_endpoint}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "1. First, define the necessary file paths for I/O operations in `config.yml`. \n",
    "2. Then, import the necessary modules and functions from `topicgpt_python`.\n",
    "3. Store your data in `data/input` and modify the `data_sample` path in `config.yml`. \n",
    "\n",
    "- Prepare your `.jsonl` data file in the following format:\n",
    "    ```\n",
    "    {\n",
    "        \"id\": \"IDs (optional)\",\n",
    "        \"text\": \"Documents\",\n",
    "        \"label\": \"Ground-truth labels (optional)\"\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topicgpt_python import generate_topic_lvl1, refine_topics, generate_topic_lvl2, assign_topics, correct_topics\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Generation \n",
    "Generate high-level topics using `generate_topic_lvl1`. \n",
    "- Define the api type and model. \n",
    "- Define your seed topics in `prompt/seed_1.md`.\n",
    "- (Optional) Modify few-shot examples in `prompt/generation_1.txt`.\n",
    "- Expect the generated topics in `data/output/{data_name}/generation_1.md` and `data/output/{data_name}/generation_1.jsonl`.\n",
    "- Right now, early stopping is set to 100, meaning that if no new topic has been generated in the last 100 iterations, the generation process will stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_topic_lvl1(\n",
    "    \"openai\",\n",
    "    \"gpt-4o\",\n",
    "    config[\"data_sample\"],\n",
    "    config[\"generation\"][\"prompt\"],\n",
    "    config[\"generation\"][\"seed\"],\n",
    "    config[\"generation\"][\"output\"],\n",
    "    config[\"generation\"][\"topic_output\"],\n",
    "    verbose=config[\"verbose\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Refinement\n",
    "If topics are generated by a weaker model, there sometimes exist irrelevant or redundant topics. This module: \n",
    "- Merges similar topics.\n",
    "- Removes overly specific or redundant topics that occur < 1% of the time (you can skip this by setting `remove` to False in `config.yml`).\n",
    "- Expect the refined topics in `data/output/{data_name}/refinement_1.md` and `data/output/{data_name}/refinement_1.jsonl`. If nothing happens, it means that the topic list is coherent.\n",
    "- If you're unsatisfied with the refined topics, call the function again with the refined topic file and refined topic file from the previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Refine topics if needed\n",
    "if config[\"refining_topics\"]:\n",
    "    refine_topics(\n",
    "        \"openai\",\n",
    "        \"gpt-4o-mini\",\n",
    "        config[\"refinement\"][\"prompt\"],\n",
    "        config[\"generation\"][\"output\"],\n",
    "        config[\"generation\"][\"topic_output\"],\n",
    "        config[\"refinement\"][\"topic_output\"],\n",
    "        config[\"refinement\"][\"output\"],\n",
    "        verbose=config[\"verbose\"],\n",
    "        remove=config[\"refinement\"][\"remove\"],\n",
    "        mapping_file=config[\"refinement\"][\"mapping_file\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtopic Generation \n",
    "Generate subtopics using `generate_topic_lvl2`.\n",
    "- This function iterates over each high-level topic and generates subtopics based on a few example documents associated with the high-level topic.\n",
    "- Expect the generated topics in `data/output/{data_name}/generation_2.md` and `data/output/{data_name}/generation_2.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Generate subtopics\n",
    "if config[\"generate_subtopics\"]:\n",
    "    generate_topic_lvl2(\n",
    "        \"openai\",\n",
    "        \"gpt-4o\",\n",
    "        config[\"generation\"][\"topic_output\"],\n",
    "        config[\"generation\"][\"output\"],\n",
    "        config[\"generation_2\"][\"prompt\"],\n",
    "        config[\"generation_2\"][\"output\"],\n",
    "        config[\"generation_2\"][\"topic_output\"],\n",
    "        verbose=config[\"verbose\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Assignment\n",
    "Assign the generated topics to the input text using `assign_topics`. Each assignment is supported by a quote from the input text.\n",
    "- Expect the assigned topics in `data/output/{data_name}/assignment.jsonl`. \n",
    "- The model used here is often a weaker model to save cost, so the topics may not be grounded in the topic list. To correct this, use the `correct_topics` module. If there are still errors/hallucinations, run the `correct_topics` module again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment\n",
    "assign_topics(\n",
    "    \"openai\",\n",
    "    \"gpt-4o-mini\",\n",
    "    config[\"data_sample\"],\n",
    "    config[\"assignment\"][\"prompt\"],\n",
    "    config[\"assignment\"][\"output\"],\n",
    "    config[\"generation\"][\n",
    "        \"topic_output\"\n",
    "    ],  # TODO: change to generation_2 if you have subtopics, or config['refinement']['topic_output'] if you refined topics\n",
    "    verbose=config[\"verbose\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "correct_topics(\n",
    "    \"openai\",\n",
    "    \"gpt-4o-mini\",\n",
    "    config[\"assignment\"][\"output\"],\n",
    "    config[\"correction\"][\"prompt\"],\n",
    "    config[\"generation\"][\n",
    "        \"topic_output\"\n",
    "    ],  # TODO: change to generation_2 if you have subtopics, or config['refinement']['topic_output'] if you refined topics\n",
    "    config[\"correction\"][\"output\"],\n",
    "    verbose=config[\"verbose\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
